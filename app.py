# -*- coding: utf-8 -*-
"""app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16EN-ZzdrHxL62TSyNa9dIynoOE58e9e6
"""

# app.py
import streamlit as st
import pandas as pd
import joblib
from sklearn.base import BaseEstimator, TransformerMixin
import re, unicodedata
import nltk
from nltk.corpus import stopwords
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

# =======================
# Konfigurasi halaman
# =======================
st.set_page_config(
    page_title="Deteksi Email Spam",
    page_icon="ðŸ“§",
    layout="centered"
)

st.title("ðŸ“§ Deteksi Email Spam")
st.write("Pilih CV, model, masukkan teks email, lalu prediksi spam/ham.")

# =======================
# Download stopwords NLTK
# =======================
nltk.download('stopwords')

# =======================
# Pilih CV & Model
# =======================
MODEL_DIR = "models/"

models_cv3 = {
    "NB Baseline": "cv3_nb_baseline.pkl",
    "SVM Baseline": "cv3_svm_baseline.pkl",
    "NB Tuned": "cv3_nb_tuned.pkl",
    "SVM Tuned": "cv3_svm_tuned.pkl",
    "Ensemble Hard Baseline": "cv3_ensemble_hard_baseline.pkl",
    "Ensemble Soft Baseline": "cv3_ensemble_soft_baseline.pkl",
    "Ensemble Hard Tuned": "cv3_ensemble_hard_tuned.pkl",
    "Ensemble Soft Tuned": "cv3_ensemble_soft_tuned.pkl"
}

models_cv5 = {
    "NB Baseline": "cv5_nb_baseline.pkl",
    "SVM Baseline": "cv5_svm_baseline.pkl",
    "NB Tuned": "cv5_nb_tuned.pkl",
    "SVM Tuned": "cv5_svm_tuned.pkl",
    "Ensemble Hard Baseline": "cv5_ensemble_hard_baseline.pkl",
    "Ensemble Soft Baseline": "cv5_ensemble_soft_baseline.pkl",
    "Ensemble Hard Tuned": "cv5_ensemble_hard_tuned.pkl",
    "Ensemble Soft Tuned": "cv5_ensemble_soft_tuned.pkl"
}

cv_choice = st.selectbox("Pilih CV:", ["CV 3", "CV 5"])

if cv_choice == "CV 3":
    model_name = st.selectbox("Pilih model:", list(models_cv3.keys()))
    model_file = models_cv3[model_name]
else:
    model_name = st.selectbox("Pilih model:", list(models_cv5.keys()))
    model_file = models_cv5[model_name]

# =======================
# Input email
# =======================
st.subheader("Masukkan teks email:")
email_input = st.text_area("Ketik email di sini:")

# =======================
# Preprocessing (sesuai kode kamu)
# =======================
stop_factory = StopWordRemoverFactory()
stopwords_id = set(stop_factory.get_stop_words())
stopwords_en = set(stopwords.words('english'))

stemmer_factory = StemmerFactory()
stemmer = stemmer_factory.create_stemmer()

# Kamus gaul
df_kamus = pd.read_csv("data/kamus_gaul.csv")
kamus_normalisasi = dict(zip(df_kamus['slang'], df_kamus['formal']))
kamus_normalisasi["communications"] = "komunikasi"
kamus_normalisasi["university"] = "universitas"
kamus_normalisasi["mail"] = "email"
kamus_normalisasi["file"] = "berkas"

additional_stopwords = {
"gue", "viagra", "per",
"dpc", "nya", "sih",
"gas", "pana", "corp", "rice", "london","dear",
"faks", "ees", "lon", "jul","rxoo","gelling","epa"
}

stopwords_all = stopwords_id.union(stopwords_en).union(additional_stopwords)

def clean_text_model(text):
    text = text.replace("\n", " ").replace("\r", " ")
    text = re.sub(r'\n+', ' ', text)
    text = str(text).lower()
    text = re.sub(r'\b(?:https?://|www\.|http\b|https\b)\S*|\S+@\S+\b', ' ', text)
    text = re.sub(r'\b(from|to|cc|bcc|subject|subjek|re|fw|fwd)\b', ' ', text)
    text = re.sub(r'\b(com|net|org|id|edu|inc|co)\b', ' ', text)
    text = re.sub(r'\b(enron|kaminski|vince|shirley|stinson|houston|hice|adobe|john|david|hou|crenshaw|stanford|ect|ee|eb|etc)\b', ' ', text, flags=re.IGNORECASE)
    text = re.sub(r'[_\-â€-â€•]+', ' ', text)
    text = ''.join(ch for ch in text if not unicodedata.category(ch).startswith("C"))
    text = re.sub(r'[^a-z\s]', ' ', text)
    text = re.sub(r'(.)\1{2,}', r'\1', text)
    text = re.sub(r'\s+', ' ', text).strip()
    words = text.split()
    words = [kamus_normalisasi.get(w, w) for w in words]
    words = [stemmer.stem(w) for w in words]
    words = [w for w in words if w not in stopwords_all and len(w) > 2]
    return " ".join(words)

class TextPreprocessor(BaseEstimator, TransformerMixin):
    def __init__(self, kamus_normalisasi, stopwords_all, stemmer):
        self.kamus_normalisasi = kamus_normalisasi
        self.stopwords_all = stopwords_all
        self.stemmer = stemmer

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return [clean_text_model(str(x)) for x in X]

preprocess = TextPreprocessor(kamus_normalisasi, stopwords_all, stemmer)

# =======================
# Tombol Prediksi
# =======================
if st.button("ðŸ“§ Prediksi Spam/Ham"):
    if not email_input.strip():
        st.warning("Masukkan teks email terlebih dahulu!")
        st.stop()

    # Load pipeline model lengkap (preprocess + TF-IDF + model)
    model = joblib.load(MODEL_DIR + model_file)

    # Prediksi
    texts = [email_input.strip()]
    predictions = model.predict(texts)
    try:
        probabilities = model.predict_proba(texts)
    except:
        probabilities = None

    # Tampilkan hasil
    st.subheader("ðŸ“Š Hasil Prediksi")
    st.success(f"Model: **{model_name}**")
    st.markdown(f"**Kategori:** {predictions[0].upper()}")

    if probabilities is not None:
        prob_dict = dict(zip(model.classes_, probabilities[0]))
        st.markdown("**Probabilitas:**")
        for k, v in prob_dict.items():
            st.write(f"{k}: {v:.2f}")
